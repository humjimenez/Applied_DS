{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o1e_hEKvDJq"
      },
      "outputs": [],
      "source": [
        "Space X Falcon 9 First Stage Landing Prediction\n",
        "Assignment: Machine Learning Prediction\n",
        "Estimated time needed: 60 minutes\n",
        "\n",
        "Space X advertises Falcon 9 rocket launches on its website with a cost of 62 million dollars; other providers cost upward of 165 million dollars each, much of the savings is because Space X can reuse the first stage. Therefore if we can determine if the first stage will land, we can determine the cost of a launch. This information can be used if an alternate company wants to bid against space X for a rocket launch. In this lab, you will create a machine learning pipeline to predict if the first stage will land given the data from the preceding labs.\n",
        "\n",
        "Image\n",
        "\n",
        "Several examples of an unsuccessful landing are shown here:\n",
        "\n",
        "Image\n",
        "\n",
        "Most unsuccessful landings are planed. Space X; performs a controlled landing in the oceans.\n",
        "\n",
        "Objectives\n",
        "Perform exploratory Data Analysis and determine Training Labels\n",
        "\n",
        "create a column for the class\n",
        "Standardize the data\n",
        "Split into training data and test data\n",
        "-Find best Hyperparameter for SVM, Classification Trees and Logistic Regression\n",
        "\n",
        "Find the method performs best using test data\n",
        "Import Libraries and Define Auxiliary Functions\n",
        "import piplite\n",
        "await piplite.install(['numpy'])\n",
        "await piplite.install(['pandas'])\n",
        "await piplite.install(['seaborn'])\n",
        "We will import the following libraries for the lab\n",
        "\n",
        "# Pandas is a software library written for the Python programming language for data manipulation and analysis.\n",
        "import pandas as pd\n",
        "# NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
        "import numpy as np\n",
        "# Matplotlib is a plotting library for python and pyplot gives us a MatLab like plotting framework. We will use this in our plotter function to plot data.\n",
        "import matplotlib.pyplot as plt\n",
        "#Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics\n",
        "import seaborn as sns\n",
        "# Preprocessing allows us to standarsize our data\n",
        "from sklearn import preprocessing\n",
        "# Allows us to split our data into training and testing data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Allows us to test parameters of classification algorithms and find the best one\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Logistic Regression classification algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# Support Vector Machine classification algorithm\n",
        "from sklearn.svm import SVC\n",
        "# Decision Tree classification algorithm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# K Nearest Neighbors classification algorithm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "This function is to plot the confusion matrix.\n",
        "\n",
        "def plot_confusion_matrix(y,y_predict):\n",
        "    \"this function plots the confusion matrix\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "    cm = confusion_matrix(y, y_predict)\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('True labels')\n",
        "    ax.set_title('Confusion Matrix');\n",
        "    ax.xaxis.set_ticklabels(['did not land', 'land']); ax.yaxis.set_ticklabels(['did not land', 'landed'])\n",
        "    plt.show()\n",
        "Load the dataframe\n",
        "Load the data\n",
        "\n",
        "from js import fetch\n",
        "import io\n",
        "\n",
        "URL1 = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_2.csv\"\n",
        "resp1 = await fetch(URL1)\n",
        "text1 = io.BytesIO((await resp1.arrayBuffer()).to_py())\n",
        "data = pd.read_csv(text1)\n",
        "data.head()\n",
        "FlightNumber\tDate\tBoosterVersion\tPayloadMass\tOrbit\tLaunchSite\tOutcome\tFlights\tGridFins\tReused\tLegs\tLandingPad\tBlock\tReusedCount\tSerial\tLongitude\tLatitude\tClass\n",
        "0\t1\t2010-06-04\tFalcon 9\t6104.959412\tLEO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0003\t-80.577366\t28.561857\t0\n",
        "1\t2\t2012-05-22\tFalcon 9\t525.000000\tLEO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0005\t-80.577366\t28.561857\t0\n",
        "2\t3\t2013-03-01\tFalcon 9\t677.000000\tISS\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB0007\t-80.577366\t28.561857\t0\n",
        "3\t4\t2013-09-29\tFalcon 9\t500.000000\tPO\tVAFB SLC 4E\tFalse Ocean\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB1003\t-120.610829\t34.632093\t0\n",
        "4\t5\t2013-12-03\tFalcon 9\t3170.000000\tGTO\tCCAFS SLC 40\tNone None\t1\tFalse\tFalse\tFalse\tNaN\t1.0\t0\tB1004\t-80.577366\t28.561857\t0\n",
        "URL2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DS0321EN-SkillsNetwork/datasets/dataset_part_3.csv'\n",
        "resp2 = await fetch(URL2)\n",
        "text2 = io.BytesIO((await resp2.arrayBuffer()).to_py())\n",
        "X = pd.read_csv(text2)\n",
        "X.head(100)\n",
        "FlightNumber\tPayloadMass\tFlights\tBlock\tReusedCount\tOrbit_ES-L1\tOrbit_GEO\tOrbit_GTO\tOrbit_HEO\tOrbit_ISS\t...\tSerial_B1058\tSerial_B1059\tSerial_B1060\tSerial_B1062\tGridFins_False\tGridFins_True\tReused_False\tReused_True\tLegs_False\tLegs_True\n",
        "0\t1.0\t6104.959412\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "1\t2.0\t525.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "2\t3.0\t677.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "3\t4.0\t500.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "4\t5.0\t3170.000000\t1.0\t1.0\t0.0\t0.0\t0.0\t1.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\t0.0\n",
        "...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\t...\n",
        "85\t86.0\t15400.000000\t2.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
        "86\t87.0\t15400.000000\t3.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t1.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
        "87\t88.0\t15400.000000\t6.0\t5.0\t5.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
        "88\t89.0\t15400.000000\t3.0\t5.0\t2.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t1.0\t0.0\t0.0\t1.0\t0.0\t1.0\t0.0\t1.0\n",
        "89\t90.0\t3681.000000\t1.0\t5.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t1.0\t0.0\t1.0\t1.0\t0.0\t0.0\t1.0\n",
        "90 rows Ã— 83 columns\n",
        "\n",
        "TASK 1\n",
        "Create a NumPy array from the column Class in data, by applying the method to_numpy() then assign it to the variable Y,make sure the output is a Pandas series (only one bracket df['name of column']).\n",
        "\n",
        "Y = pd.Series(data['Class'].to_numpy())\n",
        "Y.head(10)\n",
        "0    0\n",
        "1    0\n",
        "2    0\n",
        "3    0\n",
        "4    0\n",
        "5    0\n",
        "6    1\n",
        "7    1\n",
        "8    0\n",
        "9    0\n",
        "dtype: int64\n",
        "TASK 2\n",
        "Standardize the data in X then reassign it to the variable X using the transform provided below.\n",
        "\n",
        "# students get this\n",
        "transform = preprocessing.StandardScaler()\n",
        "X = transform.fit(X).transform(X)\n",
        "We split the data into training and testing data using the function train_test_split. The training data is divided into validation data, a second set used for training data; then the models are trained and hyperparameters are selected using the function GridSearchCV.\n",
        "\n",
        "TASK 3\n",
        "Use the function train_test_split to split the data X and Y into training and test data. Set the parameter test_size to 0.2 and random_state to 2. The training data and test data should be assigned to the following labels.\n",
        "\n",
        "X_train, X_test, Y_train, Y_test\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "we can see we only have 18 test samples.\n",
        "\n",
        "Y_test.shape\n",
        "(18,)\n",
        "TASK 4\n",
        "Create a logistic regression object then create a GridSearchCV object logreg_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters ={'C':[0.01,0.1,1],\n",
        "             'penalty':['l2'],\n",
        "             'solver':['lbfgs']}\n",
        "parameters ={\"C\":[0.01,0.1,1],'penalty':['l2'], 'solver':['lbfgs']}# l1 lasso l2 ridge\n",
        "lr=LogisticRegression()\n",
        "logreg_cv=GridSearchCV(lr, parameters, cv=10)\n",
        "logreg_cv.fit(X_train, Y_train)\n",
        "GridSearchCV\n",
        "estimator: LogisticRegression\n",
        "\n",
        "LogisticRegression\n",
        "We output the GridSearchCV object for logistic regression. We display the best parameters using the data attribute best_params_ and the accuracy on the validation data using the data attribute best_score_.\n",
        "\n",
        "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
        "print(\"accuracy :\",logreg_cv.best_score_)\n",
        "tuned hpyerparameters :(best parameters)  {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
        "accuracy : 0.8464285714285713\n",
        "TASK 5\n",
        "Calculate the accuracy on the test data using the method score:\n",
        "\n",
        "logreg_accuracy = logreg_cv.score(X_test, Y_test)\n",
        "logreg_accuracy\n",
        "0.8333333333333334\n",
        "Lets look at the confusion matrix:\n",
        "\n",
        "logreg_yhat = logreg_cv.predict(X_test)\n",
        "plot_confusion_matrix(Y_test, logreg_yhat)\n",
        "\n",
        "Examining the confusion matrix, we see that logistic regression can distinguish between the different classes. We see that the major problem is false positives.\n",
        "\n",
        "TASK 6\n",
        "Create a support vector machine object then create a GridSearchCV object svm_cv with cv - 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters = {'kernel':('linear', 'rbf','poly','rbf', 'sigmoid'),\n",
        "              'C': np.logspace(-3, 3, 5),\n",
        "              'gamma':np.logspace(-3, 3, 5)}\n",
        "svm = SVC()\n",
        "svm_cv = GridSearchCV(svm, parameters, cv=10)\n",
        "svm_cv.fit(X_train, Y_train)\n",
        "GridSearchCV\n",
        "estimator: SVC\n",
        "\n",
        "SVC\n",
        "print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\n",
        "print(\"accuracy :\",svm_cv.best_score_)\n",
        "tuned hpyerparameters :(best parameters)  {'C': 1.0, 'gamma': 0.03162277660168379, 'kernel': 'sigmoid'}\n",
        "accuracy : 0.8482142857142856\n",
        "TASK 7\n",
        "Calculate the accuracy on the test data using the method score:\n",
        "\n",
        "svm_accuracy = svm_cv.score(X_test, Y_test)\n",
        "svm_accuracy\n",
        "0.8333333333333334\n",
        "We can plot the confusion matrix\n",
        "\n",
        "svm_yhat = svm_cv.predict(X_test)\n",
        "plot_confusion_matrix(Y_test, svm_yhat)\n",
        "\n",
        "TASK 8\n",
        "Create a decision tree classifier object then create a GridSearchCV object tree_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters = {'criterion': ['gini', 'entropy'],\n",
        "     'splitter': ['best', 'random'],\n",
        "     'max_depth': [2*n for n in range(1,10)],\n",
        "     'max_features': ['auto', 'sqrt'],\n",
        "     'min_samples_leaf': [1, 2, 4],\n",
        "     'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_cv = GridSearchCV(tree, parameters, cv=10)\n",
        "tree_cv.fit(X_train, Y_train)\n",
        "/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning:\n",
        "3240 fits failed out of a total of 6480.\n",
        "The score on these train-test partitions for these parameters will be set to nan.\n",
        "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
        "\n",
        "Below are more details about the failures:\n",
        "--------------------------------------------------------------------------------\n",
        "3240 fits failed with the following error:\n",
        "Traceback (most recent call last):\n",
        "  File \"/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
        "    estimator.fit(X_train, y_train, **fit_params)\n",
        "  File \"/lib/python3.11/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
        "    estimator._validate_params()\n",
        "  File \"/lib/python3.11/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
        "    validate_parameter_constraints(\n",
        "  File \"/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
        "    raise InvalidParameterError(\n",
        "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of DecisionTreeClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
        "\n",
        "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
        "/lib/python3.11/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.76071429 0.71964286 0.80178571 0.80535714 0.80357143 0.73392857\n",
        " 0.77857143 0.78214286 0.86071429 0.78035714 0.71785714 0.75357143\n",
        " 0.84821429 0.78928571 0.71785714 0.80535714 0.80357143 0.76071429\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.78928571 0.83392857 0.78035714 0.78928571 0.79107143 0.8625\n",
        " 0.80535714 0.81785714 0.83214286 0.85       0.775      0.81964286\n",
        " 0.77678571 0.79464286 0.81964286 0.77857143 0.74464286 0.775\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.77321429 0.73214286 0.72142857 0.84821429 0.84642857 0.83571429\n",
        " 0.73571429 0.81785714 0.80178571 0.84821429 0.77678571 0.79285714\n",
        " 0.78928571 0.71071429 0.77857143 0.6625     0.73392857 0.775\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.775      0.83214286 0.74821429 0.80357143 0.77678571 0.80535714\n",
        " 0.72678571 0.76785714 0.75357143 0.78928571 0.76071429 0.79107143\n",
        " 0.74642857 0.75535714 0.83214286 0.78214286 0.70892857 0.77857143\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.85892857 0.79107143 0.76071429 0.80357143 0.74642857 0.83571429\n",
        " 0.7625     0.81785714 0.72142857 0.79107143 0.80535714 0.81964286\n",
        " 0.80535714 0.74821429 0.83035714 0.77678571 0.81964286 0.775\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.74821429 0.80535714 0.78928571 0.77857143 0.77678571 0.74821429\n",
        " 0.7375     0.77678571 0.76071429 0.83214286 0.78928571 0.79285714\n",
        " 0.81785714 0.79464286 0.83035714 0.79285714 0.79464286 0.82321429\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.78928571 0.80714286 0.73571429 0.73571429 0.79107143 0.79107143\n",
        " 0.775      0.80357143 0.72142857 0.78928571 0.77678571 0.80357143\n",
        " 0.80178571 0.80535714 0.775      0.84642857 0.72321429 0.81785714\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.77857143 0.7625     0.76428571 0.77678571 0.73392857 0.84821429\n",
        " 0.70535714 0.70714286 0.81964286 0.83392857 0.83214286 0.77678571\n",
        " 0.73035714 0.69107143 0.80535714 0.80357143 0.7625     0.79107143\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.70714286 0.81607143 0.73571429 0.75       0.78035714 0.86071429\n",
        " 0.79464286 0.7625     0.77678571 0.84821429 0.83392857 0.8625\n",
        " 0.71964286 0.80357143 0.76607143 0.79107143 0.82142857 0.76607143\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.72142857 0.80535714 0.73392857 0.8        0.79107143 0.78928571\n",
        " 0.80714286 0.79107143 0.77857143 0.81964286 0.63928571 0.74642857\n",
        " 0.74642857 0.7375     0.85892857 0.80892857 0.775      0.74821429\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.75       0.77678571 0.83392857 0.84642857 0.80357143 0.80535714\n",
        " 0.81785714 0.73214286 0.75357143 0.80357143 0.7625     0.84642857\n",
        " 0.74821429 0.79464286 0.79464286 0.83214286 0.79285714 0.81964286\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.79107143 0.80535714 0.79107143 0.80535714 0.69821429 0.81785714\n",
        " 0.80178571 0.75357143 0.80535714 0.775      0.75       0.85892857\n",
        " 0.74821429 0.80535714 0.77321429 0.84642857 0.81785714 0.83214286\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.73571429 0.80535714 0.77678571 0.81785714 0.75178571 0.81964286\n",
        " 0.81964286 0.73392857 0.75357143 0.79285714 0.74821429 0.7625\n",
        " 0.77678571 0.79107143 0.79107143 0.79107143 0.74821429 0.78035714\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.81964286 0.77857143 0.78928571 0.80357143 0.81964286 0.76428571\n",
        " 0.79107143 0.80357143 0.81785714 0.81785714 0.79285714 0.80535714\n",
        " 0.76428571 0.775      0.80357143 0.76428571 0.69464286 0.75178571\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.80714286 0.84464286 0.70892857 0.80535714 0.84821429 0.80535714\n",
        " 0.78928571 0.79107143 0.75       0.79107143 0.875      0.80357143\n",
        " 0.83035714 0.7375     0.73392857 0.80535714 0.77678571 0.80714286\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.80714286 0.76071429 0.76428571 0.80357143 0.79107143 0.81785714\n",
        " 0.80535714 0.83392857 0.77857143 0.77678571 0.7875     0.79107143\n",
        " 0.69107143 0.84821429 0.84642857 0.80357143 0.7375     0.77678571\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.84642857 0.74821429 0.775      0.7375     0.73571429 0.75\n",
        " 0.80714286 0.81607143 0.775      0.63392857 0.81607143 0.80178571\n",
        " 0.83392857 0.76071429 0.77857143 0.81785714 0.83035714 0.74642857\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        "        nan        nan        nan        nan        nan        nan\n",
        " 0.84821429 0.76428571 0.75       0.78035714 0.80535714 0.80535714\n",
        " 0.80535714 0.80535714 0.83214286 0.74642857 0.76071429 0.79107143\n",
        " 0.78035714 0.81785714 0.80178571 0.77678571 0.83392857 0.69464286]\n",
        "  warnings.warn(\n",
        "GridSearchCV\n",
        "estimator: DecisionTreeClassifier\n",
        "\n",
        "DecisionTreeClassifier\n",
        "print(\"tuned hpyerparameters :(best parameters) \",tree_cv.best_params_)\n",
        "print(\"accuracy :\",tree_cv.best_score_)\n",
        "tuned hpyerparameters :(best parameters)  {'criterion': 'entropy', 'max_depth': 12, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10, 'splitter': 'best'}\n",
        "accuracy : 0.875\n",
        "TASK 9\n",
        "Calculate the accuracy of tree_cv on the test data using the method score:\n",
        "\n",
        "tree_accuracy = tree_cv.score(X_test, Y_test)\n",
        "tree_accuracy\n",
        "0.8888888888888888\n",
        "We can plot the confusion matrix\n",
        "\n",
        "tree_yhat = svm_cv.predict(X_test)\n",
        "plot_confusion_matrix(Y_test, tree_yhat)\n",
        "\n",
        "TASK 10\n",
        "Create a k nearest neighbors object then create a GridSearchCV object knn_cv with cv = 10. Fit the object to find the best parameters from the dictionary parameters.\n",
        "\n",
        "parameters = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "              'p': [1,2]}\n",
        "\n",
        "KNN = KNeighborsClassifier()\n",
        "knn_cv = GridSearchCV(KNN, parameters, cv=10)\n",
        "knn_cv.fit(X_train, Y_train)\n",
        "/lib/python3.11/site-packages/threadpoolctl.py:1019: RuntimeWarning: libc not found. The ctypes module in Python 3.11 is maybe too old for this OS.\n",
        "  warnings.warn(\n",
        "GridSearchCV\n",
        "estimator: KNeighborsClassifier\n",
        "\n",
        "KNeighborsClassifier\n",
        "print(\"tuned hpyerparameters :(best parameters) \",knn_cv.best_params_)\n",
        "print(\"accuracy :\",knn_cv.best_score_)\n",
        "tuned hpyerparameters :(best parameters)  {'algorithm': 'auto', 'n_neighbors': 10, 'p': 1}\n",
        "accuracy : 0.8482142857142858\n",
        "TASK 11\n",
        "Calculate the accuracy of knn_cv on the test data using the method score:\n",
        "\n",
        "knn_accuracy = knn_cv.score(X_test, Y_test)\n",
        "knn_accuracy\n",
        "0.8333333333333334\n",
        "We can plot the confusion matrix\n",
        "\n",
        "knn_yhat = knn_cv.predict(X_test)\n",
        "plot_confusion_matrix(Y_test, knn_yhat)\n",
        "\n",
        "TASK 12\n",
        "Find the method performs best:\n",
        "\n",
        "from sklearn.metrics import jaccard_score, f1_score\n",
        "\n",
        "# Examining the scores from Test sets\n",
        "jaccard_scores = [\n",
        "                  jaccard_score(Y_test, logreg_yhat, average='binary'),\n",
        "                  jaccard_score(Y_test, svm_yhat, average='binary'),\n",
        "                  jaccard_score(Y_test, tree_yhat, average='binary'),\n",
        "                  jaccard_score(Y_test, knn_yhat, average='binary'),\n",
        "                 ]\n",
        "\n",
        "f1_scores = [\n",
        "             f1_score(Y_test, logreg_yhat, average='binary'),\n",
        "             f1_score(Y_test, svm_yhat, average='binary'),\n",
        "             f1_score(Y_test, tree_yhat, average='binary'),\n",
        "             f1_score(Y_test, knn_yhat, average='binary'),\n",
        "            ]\n",
        "\n",
        "accuracy = [logreg_accuracy, svm_accuracy, tree_accuracy, knn_accuracy]\n",
        "\n",
        "scores = pd.DataFrame(np.array([jaccard_scores, f1_scores, accuracy]), index=['Jaccard_Score', 'F1_Score', 'Accuracy'] , columns=['LogReg', 'SVM', 'Tree', 'KNN'])\n",
        "scores\n",
        "LogReg\tSVM\tTree\tKNN\n",
        "Jaccard_Score\t0.800000\t0.800000\t0.800000\t0.800000\n",
        "F1_Score\t0.888889\t0.888889\t0.888889\t0.888889\n",
        "Accuracy\t0.833333\t0.833333\t0.888889\t0.833333\n",
        "Authors\n",
        "Pratiksha Verma\n",
        "\n",
        "Change Log\n",
        "Date (YYYY-MM-DD)\tVersion\tChanged By\tChange Description\n",
        "2022-11-09\t1.0\tPratiksha Verma\tConverted initial version to Jupyterlite\n",
        "IBM Corporation 2022. All rights reserved.\n",
        "Click to add a cell."
      ]
    }
  ]
}